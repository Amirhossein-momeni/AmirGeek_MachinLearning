{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d107ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# Reading Data\n",
    "######################################################################################\n",
    "\n",
    "# Reading the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"../0000_Data/01-raw/01-Ad.csv\")\n",
    "\n",
    "# Defining feature columns and target column\n",
    "x_values = df.iloc[:, 3:5].values\n",
    "y_values = df.iloc[:, -1].values    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17615cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# Preprocessing \n",
    "######################################################################################\n",
    "\n",
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.25, stratify=y_values, random_state=42)\n",
    "\n",
    "# Sclaling Features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# ===================================================================================\n",
    "# Concept: random_state (Reproducibility Parameter)\n",
    "# ===================================================================================\n",
    "# Goal: Ensures the \"random\" processes (like data splitting or shuffling)\n",
    "#       produce the exact same result every time the code runs.\n",
    "#\n",
    "# Intuition:\n",
    "# Think of it as a \"pre-defined seed\" for randomness. Without it, every\n",
    "# execution is a new random result. With it (e.g., random_state=42),\n",
    "# the randomness is locked in, making experiments reproducible.\n",
    "#\n",
    "# Importance:\n",
    "# Crucial for debugging and comparing different models fairly, because\n",
    "# the input data remains identical across runs.\n",
    "# ==================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87cfb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# Building the Model\n",
    "######################################################################################\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_model = KNeighborsClassifier(n_neighbors=7)\n",
    "knn_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ed14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# Prediction and Evaluation\n",
    "######################################################################################\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = knn_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f863036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "# Types of Evaluation Metrics in classification problems and regression problems:\n",
    "# ===================================================================================\n",
    "\n",
    "## in classification problems::\n",
    "# 1. Accuracy\n",
    "# 2. Confusion Matrix & Normalized Confusion Matrix\n",
    "# 3. Recall (Sensitivity)\n",
    "# 4. Precision (Positive Predictive Value)\n",
    "# 5. Specificity (True Negative Rate)\n",
    "# 6. F1-Score\n",
    "# 7. AUC-ROC\n",
    "# 8. Log Loss\n",
    "\n",
    "## in regression problems::\n",
    "# 1. Mean Absolute Error (MAE)\n",
    "# 2. Mean Squared Error (MSE)\n",
    "# 3. Root Mean Squared Error (RMSE)\n",
    "# 4. R-Squared\n",
    "# ==================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10916aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Evaluating the Model with accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# ===================================================================================\n",
    "# Concept: Predictions vs Actuals (y_pred & y_test)\n",
    "# ===================================================================================\n",
    "# - y_test: The \"Ground Truth\". The actual, correct labels for the test data.\n",
    "# - y_pred: The \"Model's Guess\". Labels predicted by the model for the test data.\n",
    "# Goal: Compare y_pred against y_test to evaluate model performance.\n",
    "# ===================================================================================\n",
    "\n",
    "# ===================================================================================\n",
    "# Concept: Data Balance (Balanced vs Imbalanced)\n",
    "# ===================================================================================\n",
    "# - Balanced: Classes have roughly equal sample sizes.\n",
    "# - Imbalanced: One class (Majority) dominates, while another (Minority) is rare.\n",
    "# Context: Real-world data is rarely balanced (e.g., fraud is rare).\n",
    "# ===================================================================================\n",
    "\n",
    "# ===================================================================================\n",
    "# Concept: The Problem with Accuracy\n",
    "# ===================================================================================\n",
    "# Issue: Accuracy = (Correct Predictions / Total Predictions).\n",
    "# Hazard: On imbalanced data, a model can get high accuracy simply by \n",
    "#         predicting the majority class every time, failing to detect\n",
    "#         the important minority class (e.g., diseases, fraud).\n",
    "# ==================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3461d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Evaluating the Model with Confusion Matrix & Normalized Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# ===================================================================================\n",
    "#                       Predicted Negative(0)   Predicted Positive(1)\n",
    "# Actual Negative(0)       True Negative (TN)    False Positive (FP)\n",
    "# Actual Positive(1)       False Negative (FN)   True Positive (TP)\n",
    "# ===================================================================================\n",
    "\n",
    "# ===================================================================================\n",
    "# Concept: Confusion Matrix\n",
    "# ===================================================================================\n",
    "# Definition: A 2x2 table (for binary classification) summarizing the \n",
    "#             predictions of a model compared to actual ground truth.\n",
    "#\n",
    "# Key Components:\n",
    "# - TN (True Negative): Actually Neg, Predicted Neg (Correct)\n",
    "# - TP (True Positive): Actually Pos, Predicted Pos (Correct)\n",
    "# - FP (False Positive): Actually Neg, Predicted Pos (False Alarm - Type I Error)\n",
    "# - FN (False Negative): Actually Pos, Predicted Neg (Missed Detection - Type II Error)\n",
    "#\n",
    "# Intuition:\n",
    "# It reveals not just IF the model is wrong, but HOW it is wrong. It is essential\n",
    "# for imbalanced data, whereas Accuracy alone hides critical errors.\n",
    "# ==================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd8f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Evaluating the Model with Normalized Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_normalized = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "print(\"Normalized Confusion Matrix:\")\n",
    "print(cm_normalized)\n",
    "\n",
    "# confusion matrix heatmap\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"Actual Labels\")\n",
    "plt.title(\"Normalized Confusion Matrix Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================================\n",
    "# Concept: Normalized Confusion Matrix\n",
    "# ===================================================================================\n",
    "# Definition: A confusion matrix where values are converted from absolute counts\n",
    "#             to proportions (percentages) of each class, usually by row\n",
    "#             (actual labels).\n",
    "#\n",
    "# Intuition:\n",
    "# It shows the \"rate\" of success/failure instead of the raw number.\n",
    "#\n",
    "# Importance:\n",
    "# Crucial for comparing models across datasets of different sizes, and for\n",
    "# clearly visualizing performance on imbalanced data where minority classes\n",
    "# would otherwise be invisible in raw counts.\n",
    "# ==================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f861fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Evaluating the Model with Recall (Sensitivity)\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# ===================================================================================\n",
    "# Concept: Recall (Sensitivity)\n",
    "# ===================================================================================\n",
    "# Definition: The proportion of actual positive cases that were correctly \n",
    "#             identified by the model. (TP / (TP + FN))\n",
    "#\n",
    "# Intuition:\n",
    "# It measures the model's ability to \"find\" all relevant instances. \n",
    "# It focuses on minimizing \"False Negatives\" (missing a positive case).\n",
    "#\n",
    "# Use Case:\n",
    "# Critical when missing a positive case is dangerous or costly.\n",
    "# Examples: Cancer detection (missing a tumor), Fraud detection (missing a fraud).\n",
    "#\n",
    "# Limitation:\n",
    "# Recall alone can be misleading. A model predicting \"Positive\" for everyone\n",
    "# has 100% Recall, but is useless. It must be paired with Precision.\n",
    "# ==================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ba496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Evaluating the Model with Precision(Positive Predictive Value)\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "\n",
    "# ===================================================================================\n",
    "# Concept: Precision\n",
    "# ===================================================================================\n",
    "# Definition: The proportion of positive predictions that were actually correct.\n",
    "#             (TP / (TP + FP))\n",
    "#\n",
    "# Intuition:\n",
    "# It measures the model's reliability when it predicts a positive class.\n",
    "# It focuses on minimizing \"False Positives\" (False Alarms).\n",
    "#\n",
    "# Use Case:\n",
    "# Critical when a False Positive is costly or annoying.\n",
    "# Examples: Spam filtering (not blocking important emails), Fraud detection\n",
    "# (not blocking valid credit card transactions).\n",
    "#\n",
    "# Limitation:\n",
    "# Precision alone can be misleading. A model that predicts \"Positive\" only once\n",
    "# and gets it right has 100% Precision, but it misses everything else.\n",
    "# It must be paired with Recall.\n",
    "# ==================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73749947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - Evaluating the Model with Specificity (True Negative Rate)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "\n",
    "# ===================================================================================\n",
    "# Concept: Specificity\n",
    "# ===================================================================================\n",
    "# Goal: Measures how well the model identifies \"Negative\" cases (Healthy/Not Spam).\n",
    "#\n",
    "# Intuition:\n",
    "# - High Specificity: The model is \"strict\" or \"conservative\"; it rarely \n",
    "#   mistakes a healthy person for a sick one. (Low False Positive rate).\n",
    "# - Low Specificity: The model is \"reckless\"; it wrongly flags many healthy\n",
    "#   cases as positive.\n",
    "#\n",
    "# Application:\n",
    "# Critical when a \"False Positive\" is costly or dangerous.\n",
    "# Example: Email filters (not wanting important emails in Spam) or medical\n",
    "# tests where you don't want to panic healthy people.\n",
    "# ==================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f17ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - Evaluating the Model with F1-Score\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Concept: F1-Score\n",
    "# ==============================================================================\n",
    "# Definition: The harmonic mean of Precision and Recall.\n",
    "#             Formula: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "#\n",
    "# Intuition:\n",
    "# - It balances the trade-off between Precision and Recall.\n",
    "# - F1-Score is high ONLY if both Precision and Recall are high.\n",
    "# - Penalizes extreme imbalances between the two metrics.\n",
    "#\n",
    "# Use Case:\n",
    "# - Crucial for imbalanced datasets where accuracy is misleading.\n",
    "# - Provides a single, robust metric to compare model performance.\n",
    "# =============================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
